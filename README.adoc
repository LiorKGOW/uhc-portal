= OpenShift Unified Hybrid Cloud

This repository contains the UI components for the OpenShift Unified Hybrid
Cloud site.

Building the Go parts requires checking out this repo in the correct place under GOPATH:

....
git clone https://gitlab.cee.redhat.com/service/uhc-portal $GOPATH/src/gitlab.cee.redhat.com/service/uhc-portal
# OR (clones into same place)
go get -d gitlab.cee.redhat.com/service/uhc-portal/...
....

If you don't have GOPATH set, you can pick any directory e.g.:

....
mkdir $HOME/go
export GOPATH=$HOME/go
....

== Style

To promote consistency in the code base and prevent bike-shedding over style
issues, UHC follows the JavaScript and React style guides produced by airbnb.

https://github.com/airbnb/javascript[airbnb style guides]

To guide and aid developers in style consistency, UHC uses eslint and the eslint
tools provided by airbnb.

https://eslint.org/[eslint]
https://github.com/airbnb/javascript/tree/master/packages/eslint-config-airbnb[airbnb eslint tools]

To run the linter

....
$ yarn lint
....

=== Local Linting

To use eslint tools locally (e.g. as part of your editor config), you may
need to follow the installation and usage instructions using global
installation.

https://github.com/airbnb/javascript/tree/master/packages/eslint-config-airbnb#eslint-config-airbnb-1[Installation instructions]

== Bulding

To build the application install the `yarn` tool and then run these commands:

....
$ yarn install
$ yarn build
....

To build the `backend` proxy server run the `binary` target of the _Makefile_:

....
$ make binary
....

Reminder: for this and most other Makefile targets, you need this directory in the right place under GOPATH.

== Testing

To test the application in your development environment run the following command:

....
$ yarn start
....

That will start the the Webpack development server, at http://localhost:8001.

You will also need to start start the backend proxy server, which acts as a proxy
for the _OpenID_ and API services that are required by the application. Before
starting it make sure to have an offline access token, either in the `UHC_TOKEN`
environment variable or in the `token` parameter of the configuration file:

....
export UHC_TOKEN="eyJ..."
./backend
....

To obtain the access token go to the
https://cloud.openshift.com/clusters/token.html[token page] and copy the
_refresh token_.

By default the backend proxy server will be available at http://localhost:8002,
and the default configuration of the application is already prepared to use it.

If you need to change the configuration used by this backend proxy, then create a
YAML file and specify it with the `--config` commmand line option:

....
./backend --config=my.yml
....

The content of this file should be something like this (or any subset of it):

[source,yaml]
----
listener:
  address: localhost:8002

keycloak:
  url: https://developers.redhat.com/auth/realms/rhd/protocol/openid-connect/token

proxies:
- prefix: /api/
  target: https://api.stage.openshift.com

token: eyJ...
----

Note that this `--config` option and the configuration file are optional, the
default configuration already uses `localhost`, `developers.redhat.com` and port
`8002`, and already forwards all API requests to the staging environment.

If you need to use a service located in some other place, for example if you
need to use the clusters service deployed in your local environment, you can add
an additional proxy configuration:

[source,yaml]
----
proxies:
- prefix: /api/clusters_mgmt/
  target: https://api.127.0.0.1.nip.io
----

That will forward requests starting with `/api/clusters_mgmt/` to your local
clusters service, and the rest to the staging environment.

=== Testing Without a Real Backend
Sometimes the backend might be broken, but you still want to develop the UI. For this purpose we've created
a basic mock server that sends mock data. It doesn't support all actions the real backend supports, but
it should allow you to run the UI and test basic read-only functionality.

To run it, run `mockdata/mockserver.py`.

Then in another terminal, run `yarn startmock` instead of `yarn start`, and you'll get a working UI showing mock data.

=== Chromed application
Have https://github.com/RedHatInsights/insights-proxy[insights-proxy] installed under PROXY_PATH

```shell
SPANDX_CONFIG="./profiles/local-frontend.js" bash $PROXY_PATH/scripts/run.sh
```

To pass api calls to your local application run
```shell
SPANDX_CONFIG="./profiles/local-frontend-and-api.js" bash $PROXY_PATH/scripts/run.sh
```

Once the server is running you can access your UI on https://https://ci.foo.redhat.com:1337/openshift

== Deploying

In order to deploy the application to an _OpenShift_ cluster first you need to
build the image:

....
$ make image
....

As the image isn't currently uploaded to any image repository, you will need
then to save it to a `tar` file:

....
$ make tar
....

That will generate the `openshift-unified-hybrid-cloud_portal_latest.tar`. You
need to manually copy it to the nodes of the cluster where you want to deploy
the application, and load it into the docker daemon of the node. For example,
assuming that the name of the node is `sandbox` and that you have configured SSH
access without password, you can use the `scp` and `ssh` commands:

....
$ scp openshift-unified-hybrid-cloud_portal_latest.tar sandbox:
$ ssh sadnbox docker load -i openshift-unified-hybrid-cloud_portal_latest.tar
....

Once the image is loaded, you can run `deploy` target of the `Makefile` that
processes the `template.yml` template and creates all the required _OpenShift_
objects:

....
$ make deploy
....

That will create a `unified-hybrid-cloud` namespace, and inside that namespace a
`portal-server` deployment. Check that it is running:

....
$ oc project unified-hybrid-cloud
$ oc get pods
NAME                            READY STATUS  RESTARTS AGE
portal-server-c7664975c-sx6rr   1/1   Running 0        54m
....

A route for the `cloud.127.0.0.1.nip.io` DNS domain will also be created. So you
can go with your browser to https://cloud.127.0.0.1.nip.io, and you should see
the application running. To do so you can use the `route.yml` file, and the
following command:

....
$ oc create -f route.yml
....
